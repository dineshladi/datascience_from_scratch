{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression Equation\n",
    "\\begin{equation*}\n",
    "\\hat{y} = wx + c\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quadratic Loss\n",
    "\\begin{equation}\n",
    "L(y,\\hat{y}) = \\frac{1}{N} \\sum_{i=1}^N \\big(y_i - \\hat{y_i}\\big)^2\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "L(y,\\hat{y}) = \\frac{1}{N} \\sum_{i=1}^N \\big(y_i - \\left(wx_i+c\\right)\\big)^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic_loss(w,x,y):\n",
    "    '''Quadratic loss function'''\n",
    "    h = np.dot(x,w) \n",
    "    total_loss = np.sum((y-h)**2)/len(y)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use chain rule to calculate gradient of loss\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial{L}}{\\partial{w_i}} = \\frac{\\partial{L}}{\\partial{\\hat{y}}} \\frac{\\partial{\\hat{y}}}{\\partial{w}}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial{L}}{\\partial{c}} = \\frac{\\partial{L}}{\\partial{\\hat{y}}} \\frac{\\partial{\\hat{y}}}{\\partial{c}}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Examining each factor in turn\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial{L}}{\\partial{\\hat{y_i}}} = -\\frac{2}{N}(y-\\hat{y_i}) \n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial{\\hat{y}}}{\\partial{w}}  =  x_i \n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial{\\hat{y}}}{\\partial{c}}  =  1 \n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Multiplying all the indivdual terms, you get \n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial{L}}{\\partial{w_i}} = -\\frac{2}{N}(y_i-\\hat{y})x_i\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial{L}}{\\partial{c}} = -\\frac{2}{N}(y_i-\\hat{y})\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(w,x,y):\n",
    "    '''Gradient for quadratic loss'''\n",
    "    h = np.dot(x,w) \n",
    "    grad_w = -2*np.dot(x.T,y-h)/len(y)\n",
    "    return grad_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent algorithm is used to find optimum weights (w and c) that minimizes the quadratic loss where gradient is zero\n",
    "$$w = w - \\alpha \\frac{\\partial{L}}{\\partial{w}}$$\n",
    "\n",
    "$$c = c - \\alpha \\frac{\\partial{L}}{\\partial{c}}$$\n",
    "\n",
    "where $\\alpha$ is the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(learning_rate = 0.01, n_iters = 500, loss_threshold = 0.001):\n",
    "    ## Initialize the weights with zero values\n",
    "    w = np.zeros(x.shape[1])\n",
    "\n",
    "    for epoch in range(n_iters):\n",
    "        loss = quadratic_loss(w,x,y)\n",
    "        grad_w = gradient(w,x,y)\n",
    "        w = w - learning_rate * grad_w\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Loss after iteration {epoch} is {round(loss,2)}\")\n",
    "        if loss < loss_threshold:\n",
    "            break\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w,x):\n",
    "    return np.dot(x,w) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$R^2$ is defined as percentage of variation explained by the model in the data. Higher the $R^2$ better the fit\n",
    "$$R^2 = 1 - \\frac{SSE}{SST}$$\n",
    "$$R^2 = 1 - \\frac{\\sum\\left(y-\\hat{y}\\right)^2}{\\sum\\left(y-\\bar{y}\\right)^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rsquared(preds,actual):\n",
    "    y_mean = actual.mean()\n",
    "    sse = np.sum((preds - y_mean)**2)\n",
    "    sst = np.sum((actual - y_mean)**2)\n",
    "    return 1 - (sse/sst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(379, 13)\n"
     ]
    }
   ],
   "source": [
    "X, Y = load_boston(return_X_y=True)\n",
    "x, x_test, y, y_test = train_test_split(X,Y,test_size=0.25,random_state=0)\n",
    "x = normalize(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simply our implementation, we can move the bias term ($b$) to the data side so that our equation becomes $$y = wx$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adding bias term to data \n",
    "x = np.hstack((np.ones((x.shape[0],1)),x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0 is 596.46\n",
      "Loss after iteration 100 is 67.03\n",
      "Loss after iteration 200 is 62.59\n",
      "Loss after iteration 300 is 61.14\n",
      "Loss after iteration 400 is 60.52\n",
      "Loss after iteration 500 is 60.13\n",
      "Loss after iteration 600 is 59.81\n",
      "Loss after iteration 700 is 59.53\n",
      "Loss after iteration 800 is 59.26\n",
      "Loss after iteration 900 is 59.01\n"
     ]
    }
   ],
   "source": [
    "w = update_weights(learning_rate = 0.05, n_iters = 1000, loss_threshold = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127, 14) (14,)\n"
     ]
    }
   ],
   "source": [
    "x_test = normalize(x_test)\n",
    "x_test = np.hstack((np.ones((x_test.shape[0],1)),x_test))\n",
    "print(x_test.shape,w.shape)\n",
    "preds = predict(w,x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rsquared on the test data is 0.69\n",
      "MSE on the test data is 71.73\n"
     ]
    }
   ],
   "source": [
    "r2 = rsquared(preds,y_test)\n",
    "loss = quadratic_loss(w,x_test,y_test)\n",
    "print(f\"Rsquared on the test data is {round(r2,2)}\")\n",
    "print(f\"MSE on the test data is {round(loss,2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "test_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
